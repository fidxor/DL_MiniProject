{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-large-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(1024, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=1024, out_features=16, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load('./data/model/largemodel_sample1000_epoch12.pt', map_location=torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "labels = {\"INFJ\" : 0, \"INTJ\" : 1, \"INFP\" : 2, \"INTP\" : 3, \"ENFJ\" : 4, \"ENTJ\" : 5,\n",
    "              \"ENFP\" : 6, \"ENTP\" : 7, \"ISFJ\" : 8, \"ISTJ\" : 9, \"ISFP\" : 10, \"ISTP\" : 11,\n",
    "                \"ESFJ\" : 12, \"ESTJ\" : 13, \"ESFP\" : 14, \"ESTP\" : 15}\n",
    "\n",
    "# 결과 출력을 위해 labels key와 value 바꿔주기\n",
    "resultLabels = {v:k for k,v in labels.items()}\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['type']]\n",
    "        self.texts = [tokenizer(text,\n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['posts']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def prediction(model, text):\n",
    "    #특수 문자 제거\n",
    "    text_rmv = re.sub('[-=+,#/\\?:^.@*\\\"※~ㆍ!』‘|\\(\\)\\[\\]`\\'…》\\”\\“\\’·]', ' ', text)\n",
    "    # 공백 한개로 만들기\n",
    "    new_str = ' '.join(text_rmv.split())\n",
    "    # 임의의 dataframe으로 만들기\n",
    "    text_dict = {'posts': [new_str], 'type' : ['INTP']}\n",
    "    test_data = pd.DataFrame(text_dict)\n",
    "\n",
    "    #훈련 모델에 맞는 Dataset으로 변환\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              predicted = output.argmax(dim=1)\n",
    "\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "# 입력된 텍스트가 한글인지 영어인지 판단해서 한글이 하나라도 있으면 영어로 변역한다.\n",
    "def isKorean(input_s):\n",
    "    k_count = 0\n",
    "    e_count = 0\n",
    "    for c in input_s:\n",
    "        if ord('가') <= ord(c) <= ord('힣'):\n",
    "            k_count+=1\n",
    "        elif ord('a') <= ord(c.lower()) <= ord('z'):\n",
    "            e_count+=1\n",
    "\n",
    "    return k_count > 0\n",
    "\n",
    "# 한글에서 영어로 번역\n",
    "def languageTrans(text):\n",
    "    translator = Translator()\n",
    "\n",
    "    translation = translator.translate(text, dest = 'en')\n",
    "\n",
    "    return translation.text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels = {\"INFJ\" : 0, \"INTJ\" : 1, \"INFP\" : 2, \"INTP\" : 3, \"ENFJ\" : 4, \"ENTJ\" : 5,\n",
    "              \"ENFP\" : 6, \"ENTP\" : 7, \"ISFJ\" : 8, \"ISTJ\" : 9, \"ISFP\" : 10, \"ISTP\" : 11,\n",
    "                \"ESFJ\" : 12, \"ESTJ\" : 13, \"ESFP\" : 14, \"ESTP\" : 15}\n",
    "\n",
    "INTP    24961\n",
    "INTJ    22427\n",
    "INFJ    14963\n",
    "INFP    12134\n",
    "ENTP    11725\n",
    "ENFP     6167\n",
    "ISTP     3424\n",
    "ENTJ     2955\n",
    "ESTP     1986\n",
    "ENFJ     1534\n",
    "ISTJ     1243\n",
    "ISFP      875\n",
    "ISFJ      650\n",
    "ESTJ      482\n",
    "ESFP      360\n",
    "ESFJ      181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101\n",
      "ESTJ\n"
     ]
    }
   ],
   "source": [
    "input_text = input('뭐든 입력해봐')\n",
    "\n",
    "print(len(input_text))\n",
    "\n",
    "# while True:\n",
    "#     if(len(input_text) < 500):\n",
    "#         input_text = input('에이~ 좀 더 길게 써봐')\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "if isKorean(input_text):\n",
    "    input_text = languageTrans(input_text)\n",
    "\n",
    "predicted = prediction(model, input_text)\n",
    "\n",
    "print(resultLabels.get(predicted.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>State -based and instructor full -time employe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1. In listen (current situation) \\ n due to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>I like to write this article to the president,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>Teachers' promotion system scores a variety of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>8.2 I will petition for the damage caused by t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content\n",
       "0           1  State -based and instructor full -time employe...\n",
       "1           2  1. In listen (current situation) \\ n due to th...\n",
       "2           4  I like to write this article to the president,...\n",
       "3          15  Teachers' promotion system scores a variety of...\n",
       "4          16  8.2 I will petition for the damage caused by t..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_df = pd.read_csv('testdata.csv')\n",
    "\n",
    "testdata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_lst = []\n",
    "\n",
    "for content in testdata_df['content']:\n",
    "    predicted = prediction(model, content)\n",
    "    temp_lst.append(resultLabels.get(predicted.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'ISFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'ESTJ', 'ESTJ', 'ESTJ', 'INFP', 'INFP', 'ENTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ESTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ESTJ', 'INFP', 'ISTJ', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ENTJ', 'INFP', 'ENTJ', 'INFP', 'ISFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISFP', 'ENTJ', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISFP', 'ESTJ', 'ISFP', 'ENTJ', 'ENTJ', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ENTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ISFP', 'INFP', 'ESTJ', 'INFP', 'ENTJ', 'ESFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ESTJ', 'INTJ', 'INFP', 'ISTJ', 'INFP', 'ESTJ', 'INFP', 'ISFP', 'INFP', 'ESTJ', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ISTJ', 'ESTP', 'ENTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'INFP', 'ISFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ESTJ', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ESTJ', 'ESTJ', 'ISTJ', 'INFP', 'ENTJ', 'ISTJ', 'ISTJ', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'ISFP', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ENTJ', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ESTJ', 'ENTJ', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ESTJ', 'INFP', 'ISTJ', 'INFP', 'ENTJ', 'INFP', 'ESTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ENFJ', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'ENTJ', 'ESFP', 'INFP', 'ISTJ', 'ENFJ', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'ENTJ', 'ISTJ', 'ENTJ', 'ESTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ENTJ', 'INFP', 'ENTJ', 'ISTJ', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ESTJ', 'ENTJ', 'INFP', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ENTJ', 'ENTJ', 'ESTJ', 'ISTJ', 'INFP', 'ESTJ', 'INFP', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'ISTJ', 'ISTJ', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ESTJ', 'ENTJ', 'ESTJ', 'INTJ', 'INFP', 'ENTJ', 'INFP', 'INFP', 'ESTJ', 'ESFP', 'ISFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'ISTJ', 'ESTJ', 'INFP', 'ENTJ', 'ISFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ENTJ', 'ENFJ', 'INFP', 'ISFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ESTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ISTJ', 'ESTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ESTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ISFP', 'ISTJ', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ENFJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ISFP', 'ENTJ', 'ESTJ', 'ISTJ', 'ISTJ', 'ISTJ', 'ENTJ', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ISFP', 'INFP', 'ESTJ', 'ENTJ', 'ESTJ', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ENTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ESFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ENTJ', 'INFP', 'ISFP', 'INFP', 'INFP', 'ENTJ', 'ENTJ', 'ENTJ', 'INFP', 'ISTJ', 'INFP', 'ESTJ', 'INFP', 'ESTJ', 'ESTJ', 'ENTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ESTJ', 'ISFP', 'ESTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'ISTJ', 'ENTJ', 'ISTJ', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ENTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ESTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ENFJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ISFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ENTJ', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ESFP', 'ISFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ISTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'ISTJ', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ENTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ISTJ', 'ENTJ', 'INFP', 'ISTJ', 'ENTJ', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'ISTJ', 'ISTJ', 'ENTJ', 'INFP', 'ENFJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'ENFJ', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ENTJ', 'ISTJ', 'ENTJ', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'ISTJ', 'INTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'ENTJ', 'ISFP', 'INFP', 'INFP', 'ISFP', 'INFP', 'INFP', 'ENTJ', 'ESTJ', 'ISTJ', 'ESTJ', 'ISTJ', 'INFP', 'ESTJ', 'ISTJ', 'ISTJ', 'ESTJ', 'INFP', 'ISTJ', 'INFP', 'ESTJ', 'INFP', 'ESTJ', 'ESTJ', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISFP', 'ENTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ISFP', 'ENTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'INTJ', 'ESTJ', 'ESTJ', 'INFP', 'ISTJ', 'INFP', 'ENTJ', 'ENTJ', 'INFP', 'ESTJ', 'ESTJ', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'ISFP', 'INFP', 'ISTJ', 'ISTJ', 'ISTJ', 'ISTJ', 'ISFP', 'INFP', 'INFP', 'ISFP', 'ISTJ', 'INFP', 'ENFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ENFP', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'ESTJ', 'ENTJ', 'INFP', 'ENTJ', 'ESTJ', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ISTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ENTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ISTJ', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ISTJ', 'ESTJ', 'ISTJ', 'ISFP', 'ESTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ISFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ESTJ', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ESTJ', 'ISTJ', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ISTJ', 'INFP', 'ESTP', 'INFP', 'ESTJ', 'ENTJ', 'ESTJ', 'ISTJ', 'ESTJ', 'ISTJ', 'ISTJ', 'ISTJ', 'INFP', 'ENTJ', 'ISTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ENTJ', 'ESTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ENTJ', 'ISTJ', 'ESTJ', 'ISTJ', 'ENTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISFP', 'ISTJ', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INTJ', 'ESTJ', 'ENTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ESTJ', 'ENTJ', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ISFP', 'ESTP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ISTJ', 'INFP', 'ENTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ENTJ', 'INFP', 'ESTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'ENTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISFP', 'INFP', 'ISTJ', 'ISTJ', 'ESTJ', 'ESTJ', 'ISTJ', 'ENFJ', 'INFP', 'INFP', 'ENTJ', 'ISTJ', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'ISFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'ISFP', 'ESTJ', 'ESTJ', 'ISTJ', 'INFP', 'ISTJ', 'ISTJ', 'ISTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'ISTJ', 'ENTJ', 'INFP', 'ESTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ISTJ', 'ENTJ', 'ESTJ', 'INFP', 'ISTJ', 'ENTJ', 'INFP', 'ISTJ', 'ISFP', 'INFP', 'ENTJ', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'ESTP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'ESTJ', 'ENTJ', 'INFP', 'ESTJ', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ESTJ', 'INFP', 'ESTJ', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ENTJ', 'ISFP', 'ENTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ENTJ', 'INFP', 'ESTJ', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISFP', 'INFP', 'ISTJ', 'INFP', 'ISFP', 'ISTJ', 'ISTJ', 'ESFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'ISTJ', 'ENTJ', 'ENTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ENTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'ESFP', 'ESTJ', 'ESFP', 'ISTJ', 'ENTJ', 'INFP', 'ENTJ', 'ISFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'ISTJ', 'INFP', 'ISTJ', 'ISFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ISFP', 'INFP', 'INFP', 'ENFJ', 'ISTJ', 'ESTJ', 'INFP', 'INFP', 'ISTJ', 'ESTJ', 'INFP', 'ESTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ISTJ', 'ENTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ENTJ', 'INFP', 'ESTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'INFP', 'ENTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ISTJ', 'INFP', 'ESTJ', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ISTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ISTJ', 'ENTJ', 'ESTJ', 'INFP', 'ENTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'ENFJ', 'ESTJ', 'INFP', 'ISTJ', 'ESTJ', 'ISFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ISFP', 'ESTJ', 'ESTJ', 'INFP', 'ISTJ', 'ESTP', 'INFP', 'ENFJ', 'ISTJ', 'INFP', 'INFP', 'ISTJ', 'INFP', 'INFP', 'INFP', 'ISFP', 'ENTJ', 'ESTJ', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'INFP', 'ESTJ', 'ENTJ', 'INFP', 'INFP', 'INFP', 'INFP']\n"
     ]
    }
   ],
   "source": [
    "print(temp_lst)\n",
    "\n",
    "testdata_df['type'] = temp_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>934</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>370</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  content\n",
       "type                     \n",
       "ENFJ          11       11\n",
       "ENFP           2        2\n",
       "ENTJ         158      158\n",
       "ESFP           8        8\n",
       "ESTJ         184      184\n",
       "ESTP           5        5\n",
       "INFP         934      934\n",
       "INTJ           7        7\n",
       "ISFP          48       48\n",
       "ISTJ         370      370"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_df.groupby(['type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_df.to_csv('./data/csv/national petition_MBTI.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
