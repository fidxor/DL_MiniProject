{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 원본 데이터 \n",
    "df = pd.read_csv('./data/MBTI 500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBTI별 분류한 모든데이터 원본데이터는 제외\n",
    "totaldf = pd.DataFrame({'posts' : [], 'type' : [], 'type2' : []})\n",
    "def mbtipart(lst):\n",
    "    global totaldf \n",
    "\n",
    "    for alpha in lst:\n",
    "        newdf = df[df['type'].str.contains(alpha)]\n",
    "        newdf['type2'] = alpha\n",
    "\n",
    "        totaldf = pd.concat([totaldf, newdf])\n",
    "\n",
    "mbtipart(['I', 'E', 'S', 'N', 'T', 'F', 'J', 'P']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stopword_process(text, wordslst : list):\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend([\"mbti\", \"infj\", \"intj\", \"infp\", \"intp\", \"enfj\", \"entj\", \"enfp\", \"entp\",\n",
    "                        \"isfj\", \"istj\", \"isfp\", \"istp\", \"esfj\", \"estj\", \"esfp\", \"estp\", \"personality\", \"type\"])\n",
    "    stop_words.extend(['like', 'think', 'people', 'get', 'thing', 'make', 'know', 'feel', 'one', 'go'])\n",
    "    stop_words.extend(wordslst)\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    changepost = ' '.join(s for s in tokens)    \n",
    "    \n",
    "    return changepost\n",
    "\n",
    "stop_word_df = pd.DataFrame({'posts' : [], 'type' : [], 'type2' : []})\n",
    "\n",
    "def duplication_words_remove(MBTI, wordslst):\n",
    "    global stop_word_df\n",
    "    temp_totaldf = totaldf[totaldf['type2'].str.contains(MBTI[0]) | totaldf['type2'].str.contains(MBTI[1])]\n",
    "\n",
    "    temp_totaldf['posts'] = temp_totaldf['posts'].apply(stopword_process, args = (wordslst, ))\n",
    "\n",
    "    stop_word_df = pd.concat([stop_word_df, temp_totaldf])\n",
    "\n",
    "\n",
    "MBTIlst = [['I', 'E'], \n",
    "           ['S', 'N'],\n",
    "           ['T', 'F'],\n",
    "           ['J', 'P']]\n",
    "\n",
    "remove_words = [['feel', 'time', 'say', 'good', 'would', 'really', 'want', 'way', 'see'],\n",
    "                 ['know', 'time', 'really', 'good', 'feel', 'want', 'see', 'way', 'also'],\n",
    "                 ['know', 'make', 'really', 'time', 'say', 'good', 'would', 'want', 'way', 'see'],\n",
    "                ['say', 'time', 'good', 'would', 'really', 'want', 'way', 'see', 'also']]\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    duplication_words_remove(MBTIlst[i], remove_words[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_19724\\121632436.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  nparray = np.array(remove_words)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list(['feel', 'time', 'say', 'good', 'would', 'really', 'want', 'way', 'see']),\n",
       "       list(['know', 'time', 'really', 'good', 'feel', 'want', 'see', 'way', 'also']),\n",
       "       list(['know', 'make', 'really', 'time', 'say', 'good', 'would', 'want', 'way', 'see']),\n",
       "       list(['say', 'time', 'good', 'would', 'really', 'want', 'way', 'see', 'also'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_words = [['feel', 'time', 'say', 'good', 'would', 'really', 'want', 'way', 'see'],\n",
    "                 ['know', 'time', 'really', 'good', 'feel', 'want', 'see', 'way', 'also'],\n",
    "                 ['know', 'make', 'really', 'time', 'say', 'good', 'would', 'want', 'way', 'see'],\n",
    "                ['say', 'time', 'good', 'would', 'really', 'want', 'way', 'see', 'also']]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# 원본데이터에서 중복 단어들 제거\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stopword_process(text):\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend([\"mbti\", \"infj\", \"intj\", \"infp\", \"intp\", \"enfj\", \"entj\", \"enfp\", \"entp\",\n",
    "                        \"isfj\", \"istj\", \"isfp\", \"istp\", \"esfj\", \"estj\", \"esfp\", \"estp\", \"personality\", \"type\"])\n",
    "    stop_words.extend(['like', 'think', 'people', 'get', 'thing', 'make', 'know', 'feel', 'one', 'go',\n",
    "                       'feel', 'time', 'say', 'good', 'would', 'really', 'want', 'way', 'see',\n",
    "                       'know', 'time', 'really', 'good', 'feel', 'want', 'see', 'way', 'also',\n",
    "                       'know', 'make', 'really', 'time', 'say', 'good', 'would', 'want', 'way', 'see',\n",
    "                       'say', 'time', 'good', 'would', 'really', 'want', 'way', 'see', 'also'])\n",
    "    \n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    changepost = ' '.join(s for s in tokens)    \n",
    "    \n",
    "    return changepost\n",
    "\n",
    "df['posts'] = df['posts'].apply(stopword_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('MBTI_origin_remove_stopword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_df.to_csv('MBTI_remove_stopword.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
