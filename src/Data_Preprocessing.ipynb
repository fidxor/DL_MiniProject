{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터 \n",
    "df = pd.read_csv('../data/csv/MBTI 500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든데이터\n",
    "# 데이터 불균형 문제로 알파벳별로 데이터 재구성\n",
    "totaldf = df\n",
    "def mbtipart(lst):\n",
    "    global totaldf \n",
    "\n",
    "    for alpha in lst:\n",
    "        newdf = df[df['type'].str.contains(alpha)]\n",
    "        newdf['type2'] = alpha\n",
    "\n",
    "        totaldf = pd.concat([totaldf, newdf])\n",
    "\n",
    "mbtipart(['I', 'E', 'S', 'N', 'T', 'F', 'J', 'P']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 축약어 데이터 처리\n",
    "abbreviation_df = pd.read_csv('../data/csv/ENG_abbreviation.csv')\n",
    "\n",
    "abbreviation_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stopword_process(text, wordslst : list):\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend([\"mbti\", \"infj\", \"intj\", \"infp\", \"intp\", \"enfj\", \"entj\", \"enfp\", \"entp\",\n",
    "                        \"isfj\", \"istj\", \"isfp\", \"istp\", \"esfj\", \"estj\", \"esfp\", \"estp\", \"personality\", \"type\"])\n",
    "    stop_words.extend(['like', 'think', 'people', 'get', 'thing', 'make', 'know', 'feel', 'one', 'go'])\n",
    "    stop_words.extend(wordslst)\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    changepost = ' '.join(s for s in tokens)    \n",
    "    \n",
    "    return changepost\n",
    "\n",
    "stop_word_df = pd.DataFrame({'posts' : [], 'type' : [], 'type2' : []})\n",
    "\n",
    "def duplication_words_remove(MBTI, wordslst):\n",
    "    global stop_word_df\n",
    "    temp_totaldf = totaldf[totaldf['type2'].str.contains(MBTI[0]) | totaldf['type2'].str.contains(MBTI[1])]\n",
    "\n",
    "    temp_totaldf['posts'] = temp_totaldf['posts'].apply(stopword_process, args = (wordslst, ))\n",
    "\n",
    "    stop_word_df = pd.concat([stop_word_df, temp_totaldf])\n",
    "\n",
    "\n",
    "# 알파벳별로  중복 단어 제거\n",
    "MBTIlst = [['I', 'E'], \n",
    "           ['S', 'N'],\n",
    "           ['T', 'F'],\n",
    "           ['J', 'P']]\n",
    "\n",
    "remove_words = [['feel', 'time', 'say', 'good', 'would', 'really', 'want', 'way', 'see'],\n",
    "                 ['know', 'time', 'really', 'good', 'feel', 'want', 'see', 'way', 'also'],\n",
    "                 ['know', 'make', 'really', 'time', 'say', 'good', 'would', 'want', 'way', 'see'],\n",
    "                ['say', 'time', 'good', 'would', 'really', 'want', 'way', 'see', 'also']]\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    duplication_words_remove(MBTIlst[i], remove_words[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본데이터에서 중복 단어들 제거\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stopword_process(text):\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend([\"mbti\", \"infj\", \"intj\", \"infp\", \"intp\", \"enfj\", \"entj\", \"enfp\", \"entp\",\n",
    "                        \"isfj\", \"istj\", \"isfp\", \"istp\", \"esfj\", \"estj\", \"esfp\", \"estp\", \"personality\", \"type\"])\n",
    "    stop_words.extend(['like', 'think', 'people', 'get', 'thing', 'make', 'know', 'feel', 'one', 'go',\n",
    "                       'feel', 'time', 'say', 'good', 'would', 'really', 'want', 'way', 'see',\n",
    "                       'know', 'time', 'really', 'good', 'feel', 'want', 'see', 'way', 'also',\n",
    "                       'know', 'make', 'really', 'time', 'say', 'good', 'would', 'want', 'way', 'see',\n",
    "                       'say', 'time', 'good', 'would', 'really', 'want', 'way', 'see', 'also'])\n",
    "    \n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    changepost = ' '.join(s for s in tokens)    \n",
    "    \n",
    "    return changepost\n",
    "\n",
    "df['posts'] = df['posts'].apply(stopword_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
